{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Maximiliano Lopez Salgado\n",
    "- First Commit: 2023-06-20                      #folowing ISO  8601 Format\n",
    "- Last Commit: 2023-06-20                       #folowing ISO  8601 Format\n",
    "- Description: This notebook is used to perform Data Wrangling on the Superstore dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Steps in this Data Analysis:\n",
    "\n",
    "1. **Framing the Question:** \n",
    "   - The first step towards any sort of data analysis is to ask the right question(s) from the given data. \n",
    "   - Identifying the objective of the analysis makes it easier to decide on the type(s) of data needed to draw conclusions.\n",
    "\n",
    "2. **Data Wrangling:** \n",
    "   - Data wrangling, sometimes referred to as data munging or data pre-processing, is the process of gathering, assessing, and cleaning \"raw\" data into a form suitable for analysis.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA):** \n",
    "   - Once the data is collected, cleaned, and processed, it is ready for analysis. \n",
    "   - During this phase, you can use data analysis tools and software to understand, interpret, and derive conclusions based on the requirements.\n",
    "\n",
    "4. **Drawing Conclusions:** \n",
    "   - After completing the analysis phase, the next step is to interpret the analysis and draw conclusions. \n",
    "   - Three key questions to ask at this stage:\n",
    "     - Did the analysis answer my original question?\n",
    "     - Were there any limitations in my analysis that could affect my conclusions?\n",
    "     - Was the analysis sufficient to support decision-making?\n",
    "\n",
    "5. **Communicating Results:** \n",
    "   - Once data has been explored and conclusions have been drawn, it's time to communicate the findings to the relevant audience. \n",
    "   - Effective communication can be achieved through data storytelling, writing blogs, making presentations, or filing reports.\n",
    "\n",
    "**Note:** The five steps of data analysis are not always followed linearly. The process can be iterative, with steps revisited based on new insights or requirements that arise during the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Wrangling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv files \n",
    "superstore_df = pd.read_csv('../datasets/Sample-Superstore.csv', encoding='latin1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Assessing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9994, 21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look of the data´s shape\n",
    "display(superstore_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9994 non-null   int64  \n",
      " 1   Order ID       9994 non-null   object \n",
      " 2   Order Date     9994 non-null   object \n",
      " 3   Ship Date      9994 non-null   object \n",
      " 4   Ship Mode      9994 non-null   object \n",
      " 5   Customer ID    9994 non-null   object \n",
      " 6   Customer Name  9994 non-null   object \n",
      " 7   Segment        9994 non-null   object \n",
      " 8   Country        9994 non-null   object \n",
      " 9   City           9994 non-null   object \n",
      " 10  State          9994 non-null   object \n",
      " 11  Postal Code    9994 non-null   int64  \n",
      " 12  Region         9994 non-null   object \n",
      " 13  Product ID     9994 non-null   object \n",
      " 14  Category       9994 non-null   object \n",
      " 15  Sub-Category   9994 non-null   object \n",
      " 16  Product Name   9994 non-null   object \n",
      " 17  Sales          9994 non-null   float64\n",
      " 18  Quantity       9994 non-null   int64  \n",
      " 19  Discount       9994 non-null   float64\n",
      " 20  Profit         9994 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(15)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look of the data´s info\n",
    "display(superstore_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>...</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>11/8/2016</td>\n",
       "      <td>11/11/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>...</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-BO-10001798</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Somerset Collection Bookcase</td>\n",
       "      <td>261.9600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.9136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>11/8/2016</td>\n",
       "      <td>11/11/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>...</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-CH-10000454</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
       "      <td>731.9400</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>219.5820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CA-2016-138688</td>\n",
       "      <td>6/12/2016</td>\n",
       "      <td>6/16/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>DV-13045</td>\n",
       "      <td>Darrin Van Huff</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>90036</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-LA-10000240</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
       "      <td>14.6200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.8714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>10/11/2015</td>\n",
       "      <td>10/18/2015</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>...</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-TA-10000577</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
       "      <td>957.5775</td>\n",
       "      <td>5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-383.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>10/11/2015</td>\n",
       "      <td>10/18/2015</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>...</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>OFF-ST-10000760</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>Eldon Fold 'N Roll Cart System</td>\n",
       "      <td>22.3680</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.5164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0       1  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
       "1       2  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
       "2       3  CA-2016-138688   6/12/2016   6/16/2016    Second Class    DV-13045   \n",
       "3       4  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
       "4       5  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
       "\n",
       "     Customer Name    Segment        Country             City  ...  \\\n",
       "0      Claire Gute   Consumer  United States        Henderson  ...   \n",
       "1      Claire Gute   Consumer  United States        Henderson  ...   \n",
       "2  Darrin Van Huff  Corporate  United States      Los Angeles  ...   \n",
       "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
       "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
       "\n",
       "  Postal Code  Region       Product ID         Category Sub-Category  \\\n",
       "0       42420   South  FUR-BO-10001798        Furniture    Bookcases   \n",
       "1       42420   South  FUR-CH-10000454        Furniture       Chairs   \n",
       "2       90036    West  OFF-LA-10000240  Office Supplies       Labels   \n",
       "3       33311   South  FUR-TA-10000577        Furniture       Tables   \n",
       "4       33311   South  OFF-ST-10000760  Office Supplies      Storage   \n",
       "\n",
       "                                        Product Name     Sales  Quantity  \\\n",
       "0                  Bush Somerset Collection Bookcase  261.9600         2   \n",
       "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         3   \n",
       "2  Self-Adhesive Address Labels for Typewriters b...   14.6200         2   \n",
       "3      Bretford CR4500 Series Slim Rectangular Table  957.5775         5   \n",
       "4                     Eldon Fold 'N Roll Cart System   22.3680         2   \n",
       "\n",
       "   Discount    Profit  \n",
       "0      0.00   41.9136  \n",
       "1      0.00  219.5820  \n",
       "2      0.00    6.8714  \n",
       "3      0.45 -383.0310  \n",
       "4      0.20    2.5164  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look of the data´s head\n",
    "display(superstore_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode',\n",
       "       'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State',\n",
       "       'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category',\n",
       "       'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Search for all columns names\n",
    "display(superstore_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for NULL values\n",
    "display(customers.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of information that every column has\n",
    "display(df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1  Remove irrelevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant data in the 'x' column. (i.e: \"In this case, we will drop the colums that have more null values than valid values\")\n",
    "df = df.drop(columns=['column1', 'column2'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2  Remove/replace null values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the columns with missing values \n",
    "# Check datatype of the columns\n",
    "display(df['column1'].dtype)  \n",
    "\n",
    "# Transform the columns into datetime type\n",
    "df['column1'] = pd.to_datetime(df['column1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of this columns\n",
    "mean_df = df['column1'].mean()\n",
    "display(mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imput the corresponding values to the null values in this columns with the mean (or the best method for each case)\n",
    "# check if null values exist in order_data dataset\n",
    "df.isnull().sum()['column1'].fillna(mean_df, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if null values exist in order_data dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.2 Dataset's columns types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the columns with missing values \n",
    "# Check datatype of the columns (they need to be in this case datime type)\n",
    "display(df['column1'].dtype)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of this columns\n",
    "mean_column1 = df['column1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imput the corresponding values to the null values in this columns with the mean\n",
    "df['column1'].fillna(mean_column1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again if null values exist in order_data dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Drop the duplicates, if any"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use duplicate() function to find duplicated data in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates based on all columns\n",
    "display(df[df.duplicated()].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates in df \n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Type conversion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure numbers are stored as numerical data types. A date should be stored as a date object, or a Unix timestamp (number of seconds, and so on).\n",
    "In this case we already did this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 Syntax Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are values that are significantly different from all other observations. Any data value that lies more than (1.5 * IQR) away from the Q1 and Q3 quartiles is considered an outlier.\n",
    "\n",
    "In general, an e-commerce dataset obtained from a well-functioning system is less likely to have outliers compared to datasets that involve manual data entry or measurement errors. E-commerce datasets typically capture transactional information, such as customer details, product information, and order-related data, which are less prone to outliers.\n",
    "\n",
    "However, it's still possible to have outliers in certain scenarios, such as:\n",
    "\n",
    "Data entry errors: Although automated systems minimize data entry errors, there can still be instances where incorrect or extreme values are recorded.\n",
    "\n",
    "Measurement errors: If the dataset includes measurements or quantitative data collected manually, there may be measurement errors leading to outliers.\n",
    "\n",
    "System glitches or anomalies: While rare, system glitches or anomalies can occasionally result in outliers in the data.\n",
    "\n",
    "Fraudulent activities: In some cases, fraudulent transactions or activities may introduce outliers into the dataset.\n",
    "\n",
    "Therefore, while it's reasonable to assume that the occurrence of outliers in an e-commerce dataset is relatively low, it's still advisable to examine the data and apply outlier detection techniques to ensure data quality and integrity.\n",
    "\n",
    "Remember that outlier detection is an iterative process, and there is no one-size-fits-all approach. It requires a combination of domain knowledge, data understanding, and experimentation to determine the most suitable method and threshold for your specific dataset and analysis objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to find otliers\n",
    "dataframes = [df, df1, df2, df3,\n",
    "              df4]\n",
    "\n",
    "for df in dataframes:\n",
    "    # Identify numerical columns\n",
    "    numerical_columns = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Define percentiles for outlier detection (e.g., values outside [5th percentile, 95th percentile])\n",
    "    lower_percentile = 5\n",
    "    upper_percentile = 95\n",
    "\n",
    "    for column in numerical_columns:\n",
    "        # Calculate percentiles for the column\n",
    "        lower_threshold = np.percentile(df[column], lower_percentile)\n",
    "        upper_threshold = np.percentile(df[column], upper_percentile)\n",
    "\n",
    "        # Find rows with outliers in the column\n",
    "        outlier_rows = (df[column] < lower_threshold) | (df[column] > upper_threshold)\n",
    "\n",
    "        # Print rows with outliers in the column\n",
    "        print(df[outlier_rows])\n",
    "        print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing outlier analysis we indentified that because of the nature of the info, there are not relevant outliers, althought numerically there are some that exists. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.7 In-record & cross-datasets errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These errors result from having two or more values in the same row or across datasets that contradict with each other. For example, if we have a dataset about the cost of living in cities. The total column must be equivalent to the sum of rent, transport, and food."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to export the cleaned data to perform EDA in the future\n",
    "# Specify the path to the dataset folder\n",
    "folder_path = \"../datasets/\"\n",
    "\n",
    "dataframes = [df, df1, df2, df3,\n",
    "              df4]\n",
    "\n",
    "file_names = ['df.csv', 'df1.csv', 'df3.csv',\n",
    "              'df4.csv']\n",
    "\n",
    "for df, file_name in zip(dataframes, file_names):\n",
    "    # Add the prefix \"cleaned_\" to the file name\n",
    "    cleaned_file_name = \"cleaned_\" + file_name\n",
    "    # Get the full path of the output file\n",
    "    output_file_path = os.path.join(folder_path, cleaned_file_name)\n",
    "    # Save the cleaned DataFrame to CSV\n",
    "    df.to_csv(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
